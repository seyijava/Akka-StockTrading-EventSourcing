akka {
  # Log the complete configuration at INFO level when the actor system is started.
  # This is useful when you are uncertain of what configuration is used.
  log-config-on-start = off

  # stdout-loglevel = "OFF"
  stdout-loglevel = "INFO"
  # loglevel = "OFF"
  loglevel = "INFO"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  log-dead-letters = on
  log-dead-letters-during-shutdown = off

  actor {
    provider = "cluster"
  }

   remote.artery {
    enabled = on
    transport = tcp
    canonical.port =  ${clustering.port}
    canonical.hostname =  ${clustering.ip}
  }

  cluster {
    seed-nodes = [
        "akka://"${clustering.cluster.name}"@"${clustering.seed-ip}":"${clustering.seed-port}
        ]
        auto-down-unreachable-after = 10s
    roles = ["write-side"]
  }


  actor {
    allow-java-serialization = off

   serializers {
      fst = "com.bigdataconcept.akka.stock.trade.account.util.FstSerializer"
    }

   serialization-bindings {
    "com.bigdataconcept.akka.stock.trade.account.domain.Commands$OpenAccountCommand" = fst
    "com.bigdataconcept.akka.stock.trade.account.domain.Commands$AddFundCommand" = fst
    "com.bigdataconcept.akka.stock.trade.account.domain.Commands$WithdrawalFundCommand" = fst
    "com.bigdataconcept.akka.stock.trade.account.domain.Commands$UpdateContactInfoCommand" = fst
    "com.bigdataconcept.akka.stock.trade.account.domain.Commands$AddFundCommand" = fst
    "com.bigdataconcept.akka.stock.trade.account.domain.Commands$UpdateAddressCommand" = fst
    "com.bigdataconcept.akka.stock.trade.account.domain.Commands$GetBalanceCommand" = fst
     "com.bigdataconcept.akka.stock.trade.account.domain.Event$BalanceEvent" = fst



    "com.bigdataconcept.akka.stock.trade.account.domain.Event$OpenAccountEvent" = fst
    "com.bigdataconcept.akka.stock.trade.account.domain.Event$AddFundEvent" = fst
    "com.bigdataconcept.akka.stock.trade.account.domain.Event$UpdateContactInfoEvent" = fst
    "com.bigdataconcept.akka.stock.trade.account.domain.Event$UpdateAddressEvent" = fst

    "com.bigdataconcept.akka.stock.trade.account.domain.State$AccountState" = fst
     "com.bigdataconcept.akka.stock.trade.account.domain.ApiPayload$CommandResponse" = fst
     "com.bigdataconcept.akka.stock.trade.account.domain.Event$WithdrawalEvent" = fst

      "com.bigdataconcept.akka.stock.trade.account.domain.Event$AccountCreditedEvent" = fst

       "com.bigdataconcept.akka.stock.trade.account.domain.Event$AccountDebitedEvent" = fst







   }
 }


persistence {
    journal {
      plugin = "cassandra-journal"
    }
    snapshot-store {
      plugin = "cassandra-snapshot-store"
    }
  }


}






######################################
# Kafka Configuration
######################################

akka.kafka.producer {
  # Tuning parameter of how many sends that can run in parallel.
  parallelism = 100

  # Duration to wait for `KafkaProducer.close` to finish.
  close-timeout = 60s

  # Fully qualified config path which holds the dispatcher configuration
  # to be used by the producer stages. Some blocking may occur.
  # When this value is empty, the dispatcher configured for the stream
  # will be used.
  use-dispatcher = "akka.kafka.default-dispatcher"

  # The time interval to commit a transaction when using the `Transactional.sink` or `Transactional.flow`
  # for exactly-once-semantics processing.
  eos-commit-interval = 100ms

  # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
  # can be defined in this configuration section.
  kafka-clients {

     bootstrap.servers = "192.168.2.45:9092"
  }
}


akka.kafka.consumer {
  # Tuning property of scheduled polls.
  poll-interval = 50ms

  # Tuning property of the `KafkaConsumer.poll` parameter.
  # Note that non-zero value means that blocking of the thread that
  # is executing the stage will be blocked.
  poll-timeout = 50ms

  # The stage will be await outstanding offset commit requests before
  # shutting down, but if that takes longer than this timeout it will
  # stop forcefully.
  stop-timeout = 30s

  # How long to wait for `KafkaConsumer.close`
  close-timeout = 20s

  # If offset commit requests are not completed within this timeout
  # the returned Future is completed `TimeoutException`.
  commit-timeout = 15s

  # If the KafkaConsumer can't connect to the broker the poll will be
  # aborted after this timeout. The KafkaConsumerActor will throw
  # org.apache.kafka.common.errors.WakeupException, which can be handled
  # with Actor supervision strategy.
  wakeup-timeout = 10s

  # Fully qualified config path which holds the dispatcher configuration
  # to be used by the KafkaConsumerActor. Some blocking may occur.
  use-dispatcher = "akka.kafka.default-dispatcher"

  # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
  # can be defined in this configuration section.
  kafka-clients {
    # Disable auto-commit by default
    enable.auto.commit = false
     bootstrap.servers = "192.168.2.45:9092"

  }
}


account.cassandra.keyspace = account

######################################
# Persistence (Cassandra) Configuration
######################################
cassandra-journal {

  keyspace = ${account.cassandra.keyspace}
  keyspace-autocreate = true
  tables-autocreate = true

  # FQCN of the cassandra journal plugin
  class = "akka.persistence.cassandra.journal.CassandraJournal"

  # Comma-separated list of contact points in the cluster
  contact-points = [${cassandra.ip}]

  # Port of contact points in the cluster
  port = 9042

   authentication.username = "cassandra"
   authentication.password = "cassandra"
  }


  cassandra-snapshot-store {
   # Comma-separated list of contact points in the cluster
   keyspace = ${account.cassandra.keyspace}
   keyspace-autocreate = true
   tables-autocreate = true
  contact-points = [${cassandra.ip}]

  # Port of contact points in the cluster
  port = 9042

   authentication.username = "cassandra"
   authentication.password = "cassandra"

 }

######################################
# App Configuration
######################################


rest
{
     host = "127.0.0.1"
     host = ${?SERVER_IP}

     port = "6000"
     port = ${?SERVER_PORT}
}




clustering {
  ip = "127.0.0.1"
  ip = ${?CLUSTER_IP}

  port = 2554
  port = ${?CLUSTER_PORT}

  seed-ip = "127.0.0.1"
  seed-ip = ${?CLUSTER_SEED_IP}

  seed-port = 2554
  seed-port = ${?CLUSTER_SEED_PORT}

  cluster.name = "Account-Cluster"
}

cassandra
{
   ip = "192.168.2.45"
   ip = ${?CASSANDRA_IP}

   port = 9042
   port = ${?CASSANDRA_PORT}
}

kafka{

 brokerUrl = "192.168.2.45:9092"
 brokerUrl = ${?KAFKA_SERVER}


 inboundAccountTopic = "InboundAccountEventTopic"
 inboundAccountTopic = ${?kAFKA_INBOUND_TRADEORDER_TOPIC}
 inboundAccountGroupId = "PortfolioGrp"

 outboundAccountTopic = "InboundPortfolioEventTopic"
 outboundAccountTopic = ${?kAFKA_OUTBOUND_TRADEORDER_TOPIC}



}
